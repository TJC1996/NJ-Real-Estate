{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21cb9be1-8606-4615-beff-4045a0ba95de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available columns in the DataFrame:\n",
      "Index(['Municipality', 'Block', 'Lot', 'Qual', 'Property Location',\n",
      "       'Property Class', 'Owner's Name', 'Owner's Mailing Address',\n",
      "       'City/State/Zip', 'Sq. Ft.', 'Yr. Built', 'Building Class',\n",
      "       'Prior Block', 'Prior Lot', 'Prior Qual', 'Updated', 'Zone', 'Account',\n",
      "       'Mortgage Account', 'Bank Code', 'Sp Tax Cd', 'Sp Tax Cd.1',\n",
      "       'Sp Tax Cd.2', 'Sp Tax Cd.3', 'Map Page', 'Additional Lots',\n",
      "       'Land Desc', 'Building Desc', 'Class 4 Code', 'Acreage', 'EPL Own',\n",
      "       'EPL Use', 'EPL Desc', 'EPL Statute', 'EPL Init', 'EPL Further',\n",
      "       'EPL Facility Name', 'Taxes 1', 'Taxes 2', 'Taxes 3', 'Taxes 4',\n",
      "       'Sale Date', 'Deed Book', 'Deed Page', 'Sale Price', 'NU Code', 'Ratio',\n",
      "       'Type/Use', 'Year', 'Owner', 'Street', 'City/State/Zip.1',\n",
      "       'Land Assmnt', 'Building Assmnt', 'Exempt', 'Total Assmnt', 'Assessed',\n",
      "       'Year.1', 'Owner.1', 'Street.1', 'City/State/Zip.2', 'Land Assmnt.1',\n",
      "       'Building Assmnt.1', 'Exempt.1', 'Total Assmnt.1', 'Assessed.1',\n",
      "       'Year.2', 'Owner.2', 'Street.2', 'City/State/Zip.3', 'Land Assmnt.2',\n",
      "       'Building Assmnt.2', 'Exempt.2', 'Total Assmnt.2', 'Assessed.2',\n",
      "       'Year.3', 'Owner.3', 'Street.3', 'City/State/Zip.4', 'Land Assmnt.3',\n",
      "       'Building Assmnt.3', 'Exempt.3', 'Total Assmnt.3', 'Assessed.3',\n",
      "       'Latitude', 'Longitude', 'Neigh', 'VCS', 'StyDesc', 'Style', 'Join',\n",
      "       'Unnamed: 91'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with low_memory=False\n",
    "file_path = \"../data/commericalnj.csv\"\n",
    "df2 = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Display the first few rows\n",
    "df2.head()\n",
    "\n",
    "# Print the available columns in the DataFrame\n",
    "print(\"\\nAvailable columns in the DataFrame:\")\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebdd022-2b36-4f2a-9208-b88af0e24cd6",
   "metadata": {},
   "source": [
    "\n",
    "# Scalable Prototype for Commercial Property Price Prediction Using Spark\n",
    "\n",
    "This notebook demonstrates how we scale our machine learning prototype to large datasets using Apache Spark. In this prototype, we use Spark ML to build a regression pipeline with a Gradient-Boosted Tree (GBT) model. We also explain the trade-offs of distributed computing and address common schema issues with CSV data.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup Spark and Load Data\n",
    "\n",
    "We first install PySpark and create a SparkSession. We then load our CSV file (which contains our commercial property data) with the header and inferred schema.\n",
    "\n",
    "```python\n",
    "!pip install pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, sin, cos, lit, log1p, expr\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CommercialPropertyScaling\").getOrCreate()\n",
    "\n",
    "# Adjust file path as needed\n",
    "file_path = \"../data/commericalnj.csv\"\n",
    "df_spark = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Select Only the Needed Columns\n",
    "\n",
    "Our CSV file has extra columns and slight differences in header names (for example, the header uses `Total Assmnt` rather than `Total Assmnt55`). To avoid schema mismatches and header warnings, we explicitly select only the columns needed for our model. We then rename columns that contain spaces or punctuation.\n",
    "\n",
    "```python\n",
    "# Select only the necessary columns from the CSV.\n",
    "df_spark = df_spark.select(\n",
    "    \"Municipality\", \n",
    "    \"Block\", \n",
    "    \"Lot\", \n",
    "    \"Qual\", \n",
    "    \"Property Location\", \n",
    "    \"Property Class\", \n",
    "    \"Sq. Ft.\", \n",
    "    \"Yr. Built\", \n",
    "    \"Acreage\", \n",
    "    \"Total Assmnt\",   # Select the header as it appears in the CSV\n",
    "    \"Taxes 1\", \n",
    "    \"Sale Date\", \n",
    "    \"Sale Price\", \n",
    "    \"Type/Use\", \n",
    "    \"Neigh\", \n",
    "    \"Latitude\", \n",
    "    \"Longitude\"\n",
    ")\n",
    "\n",
    "# Rename problematic columns to remove spaces and punctuation.\n",
    "df_spark = df_spark.withColumnRenamed(\"Sq. Ft.\", \"Sq_Ft\") \\\n",
    "                   .withColumnRenamed(\"Yr. Built\", \"Yr_Built\") \\\n",
    "                   .withColumnRenamed(\"Total Assmnt\", \"Total_Assmnt\") \\\n",
    "                   .withColumnRenamed(\"Taxes 1\", \"Taxes_1\")\n",
    "```\n",
    "\n",
    "*Note:* Although Spark warns that the header does not conform to the schema (because of extra columns or slight naming differences), explicitly selecting only the needed columns and renaming them ensures consistency throughout our code.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Data Cleaning & Feature Engineering\n",
    "\n",
    "We now perform several data cleaning and feature engineering steps:\n",
    "\n",
    "- **Filtering:** We remove transactions with a sale price below \\$500K and remove extreme outliers (keeping only properties at or below the 95th percentile of sale price).\n",
    "- **Date Features:** We convert the \"Sale Date\" column to a timestamp and extract the sale year and month. We then create cyclic features (sine and cosine) for the sale month to capture seasonality.\n",
    "- **Building Age:** We calculate the building age using the \"Yr_Built\" column.\n",
    "- **Target Variable:** We create a log-transformed target variable (`Log_Sale_Price`) to help stabilize variance.\n",
    "\n",
    "```python\n",
    "# Filter out transactions with a Sale Price below $500K.\n",
    "df_spark = df_spark.filter(col(\"Sale Price\") >= 500000)\n",
    "\n",
    "# Remove extreme outliers: keep properties at or below the 95th percentile.\n",
    "quantiles = df_spark.approxQuantile(\"Sale Price\", [0.95], 0.05)\n",
    "if quantiles:\n",
    "    upper_threshold = quantiles[0]\n",
    "    df_spark = df_spark.filter(col(\"Sale Price\") <= upper_threshold)\n",
    "else:\n",
    "    print(\"Warning: No quantiles computed; check your 'Sale Price' column.\")\n",
    "\n",
    "# Convert Sale Date to timestamp and extract date features.\n",
    "df_spark = df_spark.withColumn(\"Sale_Date\", col(\"Sale Date\").cast(\"timestamp\"))\n",
    "df_spark = df_spark.withColumn(\"Sale_Year\", year(col(\"Sale_Date\")))\n",
    "df_spark = df_spark.withColumn(\"Sale_Month\", month(col(\"Sale_Date\")))\n",
    "# Create cyclic features for Sale Month.\n",
    "df_spark = df_spark.withColumn(\"Sale_Month_Sine\", sin(2 * 3.14159 * col(\"Sale_Month\") / 12))\n",
    "df_spark = df_spark.withColumn(\"Sale_Month_Cosine\", cos(2 * 3.14159 * col(\"Sale_Month\") / 12))\n",
    "\n",
    "# Compute Building Age using the renamed \"Yr_Built\" column.\n",
    "current_year = 2023\n",
    "df_spark = df_spark.withColumn(\"Building_Age\", lit(current_year) - col(\"Yr_Built\"))\n",
    "\n",
    "# Create the log-transformed target variable.\n",
    "df_spark = df_spark.withColumn(\"Log_Sale_Price\", log1p(col(\"Sale Price\")))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Feature Selection & Categorical Indexing\n",
    "\n",
    "We define our feature lists:\n",
    "\n",
    "- **Numerical features:** Use the renamed columns.\n",
    "- **Categorical features:** We use columns such as \"Municipality\", \"Property Class\", \"Type/Use\", and \"Neigh\".\n",
    "\n",
    "We then create a pipeline for encoding the categorical features and assembling all features into a single vector. We set `handleInvalid=\"skip\"` in the `VectorAssembler` to drop any rows that have null values in the feature columns.\n",
    "\n",
    "```python\n",
    "# Define numerical features.\n",
    "numerical_cols = [\"Sq_Ft\", \"Acreage\", \"Building_Age\", \"Sale_Year\", \n",
    "                  \"Sale_Month_Sine\", \"Sale_Month_Cosine\", \"Latitude\", \"Longitude\", \"Total_Assmnt\", \"Taxes_1\"]\n",
    "\n",
    "# Define categorical features.\n",
    "categorical_cols = [\"Municipality\", \"Property Class\", \"Type/Use\", \"Neigh\"]\n",
    "\n",
    "# Create StringIndexers for categorical features.\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_Index\", handleInvalid=\"skip\") for col in categorical_cols]\n",
    "\n",
    "# Create OneHotEncoders for the indexed categorical features.\n",
    "encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=indexer.getOutputCol() + \"_OHE\") for indexer in indexers]\n",
    "\n",
    "# Assemble all features into a single vector.\n",
    "assembler_inputs = numerical_cols + [encoder.getOutputCol() for encoder in encoders]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Define the Regression Model\n",
    "\n",
    "We use Spark’s Gradient-Boosted Tree Regressor (GBTRegressor) to predict the log-transformed sale price.\n",
    "\n",
    "```python\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"Log_Sale_Price\", maxIter=100)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Create the Pipeline\n",
    "\n",
    "We build a pipeline that includes the indexing, encoding, feature assembling, and regression model.\n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, gbt])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Split the Data into Training and Test Sets\n",
    "\n",
    "We split our data with an 80/20 ratio.\n",
    "\n",
    "```python\n",
    "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=42)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Train the Model\n",
    "\n",
    "We train our pipeline on the training data.\n",
    "\n",
    "```python\n",
    "model = pipeline.fit(train_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Evaluate the Model\n",
    "\n",
    "We evaluate our model by computing the RMSE on the log scale and then converting predictions back to the dollar scale. We also compute the R² value.\n",
    "\n",
    "```python\n",
    "# Generate predictions on the test data.\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate RMSE in log scale.\n",
    "evaluator_log = RegressionEvaluator(labelCol=\"Log_Sale_Price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_log = evaluator_log.evaluate(predictions)\n",
    "print(\"Test RMSE (log scale):\", rmse_log)\n",
    "\n",
    "# Convert predictions back to dollar scale.\n",
    "predictions = predictions.withColumn(\"Predicted_Sale_Price\", expr(\"exp(prediction) - 1\"))\n",
    "predictions = predictions.withColumn(\"Actual_Sale_Price\", col(\"Sale Price\"))\n",
    "evaluator_dollar = RegressionEvaluator(labelCol=\"Actual_Sale_Price\", predictionCol=\"Predicted_Sale_Price\", metricName=\"rmse\")\n",
    "rmse_dollar = evaluator_dollar.evaluate(predictions)\n",
    "print(\"Test RMSE (dollar scale):\", rmse_dollar)\n",
    "\n",
    "# Optionally, compute R² on the log scale.\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"Log_Sale_Price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2_log = evaluator_r2.evaluate(predictions)\n",
    "print(\"Test R² (log scale):\", r2_log)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Trade-offs and Discussion\n",
    "\n",
    "In your project documentation, explain the following trade-offs:\n",
    "- **Scalability:**  \n",
    "  Spark enables processing of billions of rows via distributed computing. However, it introduces overhead (e.g., data partitioning and shuffling) compared to a single-machine approach.\n",
    "- **Schema Challenges:**  \n",
    "  Real-world CSV files often have inconsistent headers. We explicitly selected and renamed columns to ensure a consistent schema.\n",
    "- **Algorithm Choice:**  \n",
    "  We selected the GBTRegressor from Spark ML because it can scale across large datasets, even though tuning options may differ from those available in single-machine libraries.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Stop the SparkSession\n",
    "\n",
    "Always stop your SparkSession at the end of your processing to release resources.\n",
    "\n",
    "```python\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e3d17f-69f4-40d4-b668-c720bde5a104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/anaconda3/lib/python3.12/site-packages (3.5.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/anaconda3/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/07 17:32:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/07 17:32:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///Users/anthonyclark/Desktop/DesktopFolder/realestate-price-tool/data/commericalnj.csv\n",
      "25/02/07 17:32:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///Users/anthonyclark/Desktop/DesktopFolder/realestate-price-tool/data/commericalnj.csv\n",
      "25/02/07 17:32:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///Users/anthonyclark/Desktop/DesktopFolder/realestate-price-tool/data/commericalnj.csv\n",
      "25/02/07 17:32:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///Users/anthonyclark/Desktop/DesktopFolder/realestate-price-tool/data/commericalnj.csv\n",
      "25/02/07 17:32:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///Users/anthonyclark/Desktop/DesktopFolder/realestate-price-tool/data/commericalnj.csv\n",
      "25/02/07 17:32:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///Users/anthonyclark/Desktop/DesktopFolder/realestate-price-tool/data/commericalnj.csv\n",
      "25/02/07 17:32:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///Users/anthonyclark/Desktop/DesktopFolder/realestate-price-tool/data/commericalnj.csv\n",
      "25/02/07 17:32:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///Users/anthonyclark/Desktop/DesktopFolder/realestate-price-tool/data/commericalnj.csv\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/opt/anaconda3/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.4.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "25/02/07 17:33:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///Users/anthonyclark/Desktop/DesktopFolder/realestate-price-tool/data/commericalnj.csv\n",
      "25/02/07 17:33:13 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/02/07 17:33:14 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///Users/anthonyclark/Desktop/DesktopFolder/realestate-price-tool/data/commericalnj.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE (log scale): 0.5324013761108961\n",
      "Test RMSE (dollar scale): 4176807.224328225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 17:33:14 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///Users/anthonyclark/Desktop/DesktopFolder/realestate-price-tool/data/commericalnj.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R² (log scale): 0.6642738898370417\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. Setup Spark and Load Data\n",
    "# -----------------------------\n",
    "!pip install pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, sin, cos, lit, log1p, expr\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CommercialPropertyScaling\").getOrCreate()\n",
    "\n",
    "# Adjust file path as needed\n",
    "file_path = \"../data/commericalnj.csv\"\n",
    "df_spark = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 1.1 Rename Problematic Columns\n",
    "# -----------------------------\n",
    "# Rename columns to remove spaces and punctuation.\n",
    "# (Make sure these names exactly match those in your CSV header.)\n",
    "df_spark = df_spark.withColumnRenamed(\"Sq. Ft.\", \"Sq_Ft\") \\\n",
    "                   .withColumnRenamed(\"Yr. Built\", \"Yr_Built\") \\\n",
    "                   .withColumnRenamed(\"Total Assmnt55\", \"Total_Assmnt\") \\\n",
    "                   .withColumnRenamed(\"Taxes 1\", \"Taxes_1\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1.2 Select Only the Needed Columns\n",
    "# -----------------------------\n",
    "# Use the renamed column names here.\n",
    "df_spark = df_spark.select(\n",
    "    \"Municipality\", \n",
    "    \"Block\", \n",
    "    \"Lot\", \n",
    "    \"Qual\", \n",
    "    \"Property Location\", \n",
    "    \"Property Class\", \n",
    "    \"Sq_Ft\", \n",
    "    \"Yr_Built\", \n",
    "    \"Acreage\", \n",
    "    \"Total_Assmnt\", \n",
    "    \"Taxes_1\", \n",
    "    \"Sale Date\", \n",
    "    \"Sale Price\", \n",
    "    \"Type/Use\", \n",
    "    \"Neigh\", \n",
    "    \"Latitude\", \n",
    "    \"Longitude\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Data Cleaning & Feature Engineering\n",
    "# -----------------------------\n",
    "# Filter out transactions with a Sale Price below $500K.\n",
    "df_spark = df_spark.filter(col(\"Sale Price\") >= 500000)\n",
    "\n",
    "# Remove extreme outliers: keep properties at or below the 95th percentile.\n",
    "quantiles = df_spark.approxQuantile(\"Sale Price\", [0.95], 0.05)\n",
    "if quantiles:\n",
    "    upper_threshold = quantiles[0]\n",
    "    df_spark = df_spark.filter(col(\"Sale Price\") <= upper_threshold)\n",
    "else:\n",
    "    print(\"Warning: No quantiles computed; check your 'Sale Price' column.\")\n",
    "\n",
    "# Convert Sale Date to timestamp and extract date features.\n",
    "df_spark = df_spark.withColumn(\"Sale_Date\", col(\"Sale Date\").cast(\"timestamp\"))\n",
    "df_spark = df_spark.withColumn(\"Sale_Year\", year(col(\"Sale_Date\")))\n",
    "df_spark = df_spark.withColumn(\"Sale_Month\", month(col(\"Sale_Date\")))\n",
    "# Create cyclic features for Sale Month to capture seasonality.\n",
    "df_spark = df_spark.withColumn(\"Sale_Month_Sine\", sin(2 * 3.14159 * col(\"Sale_Month\") / 12))\n",
    "df_spark = df_spark.withColumn(\"Sale_Month_Cosine\", cos(2 * 3.14159 * col(\"Sale_Month\") / 12))\n",
    "\n",
    "# Compute Building Age using the renamed \"Yr_Built\" column.\n",
    "current_year = 2023  # You can derive this programmatically if needed.\n",
    "df_spark = df_spark.withColumn(\"Building_Age\", lit(current_year) - col(\"Yr_Built\"))\n",
    "\n",
    "# Create the log-transformed target variable.\n",
    "df_spark = df_spark.withColumn(\"Log_Sale_Price\", log1p(col(\"Sale Price\")))\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Feature Selection & Categorical Indexing\n",
    "# -----------------------------\n",
    "# Define numerical features (using renamed column names).\n",
    "numerical_cols = [\"Sq_Ft\", \"Acreage\", \"Building_Age\", \"Sale_Year\", \n",
    "                  \"Sale_Month_Sine\", \"Sale_Month_Cosine\", \"Latitude\", \"Longitude\", \"Total_Assmnt\", \"Taxes_1\"]\n",
    "\n",
    "# Define categorical features.\n",
    "categorical_cols = [\"Municipality\", \"Property Class\", \"Type/Use\", \"Neigh\"]\n",
    "\n",
    "# Create StringIndexers and OneHotEncoders for each categorical column.\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_Index\", handleInvalid=\"skip\") for col in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=indexer.getOutputCol() + \"_OHE\") for indexer in indexers]\n",
    "\n",
    "# Assemble all features into a single feature vector.\n",
    "assembler_inputs = numerical_cols + [encoder.getOutputCol() for encoder in encoders]\n",
    "# Set handleInvalid=\"skip\" to drop rows with null values in any feature column.\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Define the Regression Model\n",
    "# -----------------------------\n",
    "# Using Spark's Gradient-Boosted Tree Regressor.\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"Log_Sale_Price\", maxIter=100)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Create a Pipeline\n",
    "# -----------------------------\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, gbt])\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Split Data into Training and Test Sets\n",
    "# -----------------------------\n",
    "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Train the Model\n",
    "# -----------------------------\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Evaluate the Model\n",
    "# -----------------------------\n",
    "# Evaluate RMSE in log scale.\n",
    "predictions = model.transform(test_data)\n",
    "evaluator_log = RegressionEvaluator(labelCol=\"Log_Sale_Price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_log = evaluator_log.evaluate(predictions)\n",
    "print(\"Test RMSE (log scale):\", rmse_log)\n",
    "\n",
    "# Convert predictions back to dollar scale for evaluation.\n",
    "predictions = predictions.withColumn(\"Predicted_Sale_Price\", expr(\"exp(prediction) - 1\"))\n",
    "predictions = predictions.withColumn(\"Actual_Sale_Price\", col(\"Sale Price\"))\n",
    "evaluator_dollar = RegressionEvaluator(labelCol=\"Actual_Sale_Price\", predictionCol=\"Predicted_Sale_Price\", metricName=\"rmse\")\n",
    "rmse_dollar = evaluator_dollar.evaluate(predictions)\n",
    "print(\"Test RMSE (dollar scale):\", rmse_dollar)\n",
    "\n",
    "# Optionally, compute R² on the log scale.\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"Log_Sale_Price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2_log = evaluator_r2.evaluate(predictions)\n",
    "print(\"Test R² (log scale):\", r2_log)\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Trade-offs and Discussion\n",
    "# -----------------------------\n",
    "# Scaling to large data with Spark involves trade-offs:\n",
    "# - Distributed computing enables processing billions of rows, but adds overhead from data partitioning and shuffling.\n",
    "# - Algorithms must be designed to minimize communication overhead.\n",
    "# - Spark ML's algorithms may not be as finely tuned as their single-machine counterparts,\n",
    "#   but they offer scalability that is essential for web-scale data.\n",
    "#\n",
    "# In your documentation, explain these trade-offs and why you selected Spark ML and GBTRegressor\n",
    "# for your scalable prototype.\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Stop the SparkSession\n",
    "# -----------------------------\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a95a1-749a-47dc-acd3-58df71563590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
